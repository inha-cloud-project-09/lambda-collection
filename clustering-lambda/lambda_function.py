import json
import boto3
import pymysql
import os
import random
import math
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

sns_client = boto3.client("sns")

# í™˜ê²½ ë³€ìˆ˜
DB_HOST = os.environ["DB_HOST"]
DB_USER = os.environ["DB_USER"]
DB_PASSWORD = os.environ["DB_PASSWORD"]
DB_NAME = os.environ["DB_NAME"]
SNS_TOPIC_ARN = os.environ.get('SNS_TOPIC_ARN')

def get_db_connection():
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
    return pymysql.connect(
        host=DB_HOST,
        user=DB_USER,
        password=DB_PASSWORD,
        database=DB_NAME,
        charset="utf8mb4",
        cursorclass=pymysql.cursors.DictCursor,
    )


def publish_clustering_completion(total_diaries, total_users, clusters_created):
    try:
        if not SNS_TOPIC_ARN:
            logger.warning("SNS_TOPIC_ARN í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
            
        current_time = datetime.now()
        message = {
            "totalDiaries": total_diaries,
            "totalUsers": total_users,
            "clustersCreated": clusters_created,
            "event": "batch_clustering_completed",
            "completedAt": current_time.isoformat(),
            "completedAtKST": (current_time + timedelta(hours=9)).strftime('%Y-%m-%d %H:%M:%S')
        }
        
        sns_client.publish(
            TopicArn=SNS_TOPIC_ARN,
            Message=json.dumps(message),
            Subject='batch_clustering_completed'
        )
        logger.info(f"í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ SNS ë©”ì‹œì§€ ë°œí–‰: {total_users}ëª… ì²˜ë¦¬ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"SNS ë°œí–‰ ì˜¤ë¥˜: {str(e)}")

def cosine_similarity(a, b):
    """ë‘ ë²¡í„° ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ìˆœìˆ˜ Python)"""
    dot_product = sum(x * y for x, y in zip(a, b))
    norm_a = math.sqrt(sum(x * x for x in a))
    norm_b = math.sqrt(sum(x * x for x in b))

    # ì œë¡œ ë²¡í„° ì²˜ë¦¬
    if norm_a == 0 or norm_b == 0:
        return 0.1  # ìµœì†Œê°’ ë¶€ì—¬í•˜ì—¬ ì¶”ì²œ ê°€ëŠ¥í•˜ê²Œ

    return dot_product / (norm_a * norm_b)


def euclidean_distance(a, b):
    """ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê³„ì‚°"""
    return math.sqrt(sum((x - y) ** 2 for x, y in zip(a, b)))


def simple_kmeans_python(vectors, n_clusters, max_iters=100, random_seed=42):
    """ìˆœìˆ˜ Pythonìœ¼ë¡œ êµ¬í˜„ëœ K-means"""
    n_samples = len(vectors)
    n_features = len(vectors[0]) if vectors else 0

    if n_samples < n_clusters:
        # ìƒ˜í”Œ ìˆ˜ê°€ í´ëŸ¬ìŠ¤í„° ìˆ˜ë³´ë‹¤ ì ìœ¼ë©´ ê° ìƒ˜í”Œì„ ê°œë³„ í´ëŸ¬ìŠ¤í„°ë¡œ
        labels = list(range(n_samples))
        centroids = [list(vector) for vector in vectors]
        return labels, centroids

    # ëœë¤ ì‹œë“œ ì„¤ì •
    random.seed(random_seed)

    # K-means++ ì´ˆê¸°í™”
    centroids = []
    centroids.append(list(vectors[random.randint(0, n_samples - 1)]))

    for _ in range(1, n_clusters):
        distances = []
        for vector in vectors:
            min_dist = min(
                euclidean_distance(vector, centroid) ** 2 for centroid in centroids
            )
            distances.append(min_dist)

        total_dist = sum(distances)
        if total_dist == 0:
            centroids.append(list(vectors[random.randint(0, n_samples - 1)]))
            continue

        probabilities = [d / total_dist for d in distances]
        cumulative_probs = []
        cumsum = 0
        for p in probabilities:
            cumsum += p
            cumulative_probs.append(cumsum)

        r = random.random()
        for j, p in enumerate(cumulative_probs):
            if r < p:
                centroids.append(list(vectors[j]))
                break

    # K-means ë°˜ë³µ
    for iteration in range(max_iters):
        # ê° ì ì„ ê°€ì¥ ê°€ê¹Œìš´ ì¤‘ì‹¬ì ì— í• ë‹¹
        labels = []
        for vector in vectors:
            distances = [euclidean_distance(vector, centroid) for centroid in centroids]
            closest_cluster = distances.index(min(distances))
            labels.append(closest_cluster)

        # ìƒˆë¡œìš´ ì¤‘ì‹¬ì  ê³„ì‚°
        new_centroids = []
        for k in range(n_clusters):
            cluster_points = [vectors[i] for i in range(n_samples) if labels[i] == k]

            if cluster_points:
                # í‰ê·  ê³„ì‚°
                new_centroid = []
                for feature_idx in range(n_features):
                    feature_sum = sum(point[feature_idx] for point in cluster_points)
                    new_centroid.append(feature_sum / len(cluster_points))
                new_centroids.append(new_centroid)
            else:
                # ë¹ˆ í´ëŸ¬ìŠ¤í„°ë©´ ê¸°ì¡´ ì¤‘ì‹¬ì  ìœ ì§€
                new_centroids.append(centroids[k][:])

        # ìˆ˜ë ´ ì²´í¬
        converged = True
        for old, new in zip(centroids, new_centroids):
            for old_val, new_val in zip(old, new):
                if abs(old_val - new_val) > 1e-4:
                    converged = False
                    break
            if not converged:
                break

        if converged:
            logger.info(f"K-means ìˆ˜ë ´: {iteration + 1}ë²ˆì§¸ ë°˜ë³µ")
            break

        centroids = new_centroids

    return labels, centroids


def get_completed_diaries(connection) -> List[Dict]:
    """ë¶„ì„ ì™„ë£Œëœ ëª¨ë“  ì¼ê¸° ì¡°íšŒ"""
    with connection.cursor() as cursor:
        query = """
        SELECT 
            id,
            user_id,
            emotion_vector,
            primary_emotion,
            created_at
        FROM diaries 
        WHERE analysis_status = 'completed' 
            AND emotion_vector IS NOT NULL
            AND JSON_LENGTH(emotion_vector) = 10
        ORDER BY created_at DESC
        """
        cursor.execute(query)
        return cursor.fetchall()


def parse_emotion_vectors(
    diaries: List[Dict],
) -> Tuple[List[List[float]], List[int], List[int]]:
    """ê°ì • ë²¡í„° íŒŒì‹± ë° ê²€ì¦"""
    vectors = []
    diary_ids = []
    user_ids = []

    for diary in diaries:
        try:
            # JSON ë¬¸ìì—´ì„ íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            if isinstance(diary["emotion_vector"], str):
                emotion_vector = json.loads(diary["emotion_vector"])
            else:
                emotion_vector = diary["emotion_vector"]

            # ë²¡í„° ê¸¸ì´ ê²€ì¦
            if len(emotion_vector) == 10:
                vectors.append([float(x) for x in emotion_vector])
                diary_ids.append(diary["id"])
                user_ids.append(diary["user_id"])
            else:
                logger.warning(
                    f"ì¼ê¸° {diary['id']}: ì˜ëª»ëœ ë²¡í„° ê¸¸ì´ {len(emotion_vector)}"
                )

        except (json.JSONDecodeError, TypeError, ValueError) as e:
            logger.warning(f"ì¼ê¸° {diary['id']}: ë²¡í„° íŒŒì‹± ì‹¤íŒ¨ - {str(e)}")

    return vectors, diary_ids, user_ids


def perform_clustering(
    vectors: List[List[float]], min_cluster_size: int = 3
) -> Tuple[List[int], List[List[float]]]:
    """K-means í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰"""
    n_samples = len(vectors)

    if n_samples < min_cluster_size:
        logger.warning(f"ìƒ˜í”Œ ìˆ˜ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤: {n_samples}")
        # ëª¨ë“  ì ì„ í´ëŸ¬ìŠ¤í„° 0ìœ¼ë¡œ í• ë‹¹, í‰ê· ì„ ì¤‘ì‹¬ì ìœ¼ë¡œ
        labels = [0] * n_samples
        if vectors:
            n_features = len(vectors[0])
            centroid = [
                sum(vectors[i][j] for i in range(n_samples)) / n_samples
                for j in range(n_features)
            ]
            centroids = [centroid]
        else:
            centroids = [[0.0] * 10]
        return labels, centroids

    # ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì •
    if n_samples < 10:
        n_clusters = 2
    elif n_samples < 50:
        n_clusters = min(5, n_samples // 3)
    else:
        n_clusters = min(10, n_samples // 10)

    logger.info(f"í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰: {n_samples}ê°œ ìƒ˜í”Œ, {n_clusters}ê°œ í´ëŸ¬ìŠ¤í„°")

    labels, centroids = simple_kmeans_python(
        vectors=vectors, n_clusters=n_clusters, max_iters=300, random_seed=42
    )

    return labels, centroids


def save_clustering_results(connection, centroids: List[List[float]]) -> Dict[int, int]:
    """í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
    cluster_id_mapping = {}

    with connection.cursor() as cursor:
        # ê¸°ì¡´ í´ëŸ¬ìŠ¤í„° ìºì‹œ ì‚­ì œ
        cursor.execute("DELETE FROM clustering_cache")

        # ìƒˆë¡œìš´ í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì  ì €ì¥
        for i, centroid in enumerate(centroids):
            centroid_json = json.dumps(centroid)

            cursor.execute(
                """
                INSERT INTO clustering_cache (cluster_id, centroid_vector, member_count, created_at)
                VALUES (%s, %s, %s, NOW())
            """,
                (i, centroid_json, 0),
            )

            cluster_id_mapping[i] = cursor.lastrowid

    connection.commit()
    return cluster_id_mapping


def get_user_current_communities(connection, user_id: int) -> List[int]:
    """ì‚¬ìš©ìì˜ í˜„ì¬ í™œì„± ì»¤ë®¤ë‹ˆí‹° ì¡°íšŒ"""
    with connection.cursor() as cursor:
        cursor.execute(
            """
            SELECT community_id 
            FROM community_members 
            WHERE user_id = %s AND is_active = 1
        """,
            (user_id,),
        )
        return [row["community_id"] for row in cursor.fetchall()]


def get_or_create_community(
    connection, cluster_id: int, emotion_theme: str = None
) -> int:
    """í´ëŸ¬ìŠ¤í„°ì— í•´ë‹¹í•˜ëŠ” ì»¤ë®¤ë‹ˆí‹° ì¡°íšŒ ë˜ëŠ” ìƒì„±"""
    with connection.cursor() as cursor:
        # ê¸°ì¡´ ì»¤ë®¤ë‹ˆí‹° ì¡°íšŒ
        cursor.execute(
            """
            SELECT id FROM communities 
            WHERE name LIKE %s AND is_default = 1
            ORDER BY created_at DESC
            LIMIT 1
        """,
            (f"ê°ì •ë‚˜ëˆ”ë°© #{cluster_id}%",),
        )

        result = cursor.fetchone()
        if result:
            return result["id"]

        # ìƒˆ ì»¤ë®¤ë‹ˆí‹° ìƒì„±
        community_name = f"ê°ì •ë‚˜ëˆ”ë°© #{cluster_id}"
        description = f"í´ëŸ¬ìŠ¤í„° {cluster_id}ì˜ ë¹„ìŠ·í•œ ê°ì •ì„ ê°€ì§„ ì‚¬ëŒë“¤ì˜ ë‚˜ëˆ”ë°©"

        cursor.execute(
            """
            INSERT INTO communities (name, description, creator_id, is_default, emotion_theme, created_at)
            VALUES (%s, %s, NULL, 1, %s, NOW())
        """,
            (community_name, description, emotion_theme),
        )

        community_id = cursor.lastrowid
        connection.commit()

        logger.info(f"ìƒˆ ì»¤ë®¤ë‹ˆí‹° ìƒì„±: {community_name} (ID: {community_id})")
        return community_id


def create_community_recommendation_alert(
    connection, user_id: int, community_id: int, similarity_score: float, reason: str
):
    """ì»¤ë®¤ë‹ˆí‹° ì¶”ì²œ ì•Œë¦¼ ìƒì„±"""
    with connection.cursor() as cursor:
        # ì»¤ë®¤ë‹ˆí‹° ì •ë³´ ì¡°íšŒ
        cursor.execute(
            """
            SELECT name, description FROM communities WHERE id = %s
        """,
            (community_id,),
        )
        community = cursor.fetchone()

        if not community:
            return

        message = f"""ğŸ’« ìƒˆë¡œìš´ ë‚˜ëˆ”ë°©ì„ ì¶”ì²œë“œë ¤ìš”!

ğŸ  **{community['name']}**
ğŸ“ {community['description']}

ğŸ¯ **ì¶”ì²œ ì´ìœ **: {reason}
ğŸ“Š **ê°ì • ìœ ì‚¬ë„**: {similarity_score:.1%}

ì´ ë‚˜ëˆ”ë°©ì— ì°¸ì—¬í•˜ì—¬ ë¹„ìŠ·í•œ ê°ì •ì„ ê°€ì§„ ë¶„ë“¤ê³¼ ì†Œí†µí•´ë³´ì„¸ìš”! 
ê¸°ì¡´ ë‚˜ëˆ”ë°©ì€ ê·¸ëŒ€ë¡œ ìœ ì§€ë˜ë©°, ì›í•˜ì‹œë©´ ì–¸ì œë“  ì¶”ê°€ë¡œ ì°¸ì—¬í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."""

        # ì¤‘ë³µ ì•Œë¦¼ ë°©ì§€ (ìµœê·¼ 24ì‹œê°„ ë‚´ ê°™ì€ ì»¤ë®¤ë‹ˆí‹° ì¶”ì²œ ì²´í¬)
        cursor.execute(
            """
            SELECT id FROM user_alerts 
            WHERE user_id = %s 
                AND alert_type = 'community_invite'
                AND message LIKE %s
                AND created_at > DATE_SUB(NOW(), INTERVAL 24 HOUR)
        """,
            (user_id, f"%{community['name']}%"),
        )

        if cursor.fetchone():
            logger.info(f"ì‚¬ìš©ì {user_id}ì—ê²Œ ì´ë¯¸ ìµœê·¼ ì•Œë¦¼ ë°œì†¡ë¨")
            return

        # ìƒˆ ì•Œë¦¼ ìƒì„±
        cursor.execute(
            """
            INSERT INTO user_alerts (user_id, alert_type, message, created_at, is_read)
            VALUES (%s, 'community_invite', %s, NOW(), 0)
        """,
            (user_id, message),
        )

        connection.commit()
        logger.info(
            f"ì»¤ë®¤ë‹ˆí‹° ì¶”ì²œ ì•Œë¦¼ ìƒì„±: ì‚¬ìš©ì {user_id} â†’ ì»¤ë®¤ë‹ˆí‹° {community_id}"
        )


def process_user_community_recommendations(
    connection,
    user_id: int,
    user_vector: List[float],
    centroids: List[List[float]],
    current_cluster: int,
):
    """ì‚¬ìš©ìë³„ ì»¤ë®¤ë‹ˆí‹° ì¶”ì²œ ì²˜ë¦¬"""
    try:
        # í˜„ì¬ í™œì„± ì»¤ë®¤ë‹ˆí‹° ì¡°íšŒ
        current_communities = get_user_current_communities(connection, user_id)

        # ëª¨ë“  í´ëŸ¬ìŠ¤í„°ì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = []
        for centroid in centroids:
            similarity = cosine_similarity(user_vector, centroid)
            similarities.append(similarity)

        # ìƒìœ„ 3ê°œ í´ëŸ¬ìŠ¤í„° ì„ íƒ
        cluster_scores = [(i, score) for i, score in enumerate(similarities)]
        cluster_scores.sort(key=lambda x: x[1], reverse=True)

        recommendations_made = 0
        for cluster_id, similarity_score in cluster_scores[:3]:
            if recommendations_made >= 2:  # ìµœëŒ€ 2ê°œ ì¶”ì²œ
                break

            # ìµœì†Œ ìœ ì‚¬ë„ ì¡°ê±´ (30% ì´ìƒ)
            if similarity_score < 0.05:
                continue

            # í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì˜ ì»¤ë®¤ë‹ˆí‹° ì¡°íšŒ/ìƒì„±
            community_id = get_or_create_community(connection, cluster_id)

            # ì´ë¯¸ ê°€ì…ëœ ì»¤ë®¤ë‹ˆí‹°ë©´ ìŠ¤í‚µ
            if community_id in current_communities:
                continue

            # ì¶”ì²œ ì´ìœ  ìƒì„±
            if cluster_id == current_cluster:
                reason = "í˜„ì¬ ê°ì •ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ì»¤ë®¤ë‹ˆí‹°ì…ë‹ˆë‹¤"
            elif similarity_score > 0.8:
                reason = "ê°ì • íŒ¨í„´ì´ ë§¤ìš° ìœ ì‚¬í•©ë‹ˆë‹¤"
            elif similarity_score > 0.6:
                reason = "ë¹„ìŠ·í•œ ê°ì •ì„ ìì£¼ ê²½í—˜í•©ë‹ˆë‹¤"
            elif similarity_score > 0.4:
                reason = "ê´€ì‹¬ ìˆì„ ë§Œí•œ ê°ì • ì£¼ì œì…ë‹ˆë‹¤"
            else:
                reason = "ìƒˆë¡œìš´ ê°ì • ê²½í—˜ì„ ë‚˜ëˆŒ ìˆ˜ ìˆëŠ” ì»¤ë®¤ë‹ˆí‹°ì…ë‹ˆë‹¤"

            # ì¶”ì²œ ì•Œë¦¼ ìƒì„±
            create_community_recommendation_alert(
                connection, user_id, community_id, similarity_score, reason
            )

            recommendations_made += 1

    except Exception as e:
        logger.error(f"ì‚¬ìš©ì {user_id} ì¶”ì²œ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")


def update_clustering_cache_member_counts(
    connection, labels: List[int], user_ids: List[int]
):
    """í´ëŸ¬ìŠ¤í„°ë³„ ë©¤ë²„ ìˆ˜ ì—…ë°ì´íŠ¸"""
    cluster_counts = {}
    for label in labels:
        cluster_counts[label] = cluster_counts.get(label, 0) + 1

    with connection.cursor() as cursor:
        for cluster_id, count in cluster_counts.items():
            cursor.execute(
                """
                UPDATE clustering_cache 
                SET member_count = %s 
                WHERE cluster_id = %s
            """,
                (count, cluster_id),
            )

    connection.commit()


def process_individual_analysis(
    connection, diary_id: int, user_id: int, emotion_vector: List[float]
):
    """ê°œë³„ ì¼ê¸° ë¶„ì„ ì²˜ë¦¬ (SQS íŠ¸ë¦¬ê±°ìš©)"""
    try:
        logger.info(f"ê°œë³„ ì¼ê¸° ë¶„ì„ ì‹œì‘: diary_id={diary_id}, user_id={user_id}")

        # í˜„ì¬ í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì ë“¤ ì¡°íšŒ
        with connection.cursor() as cursor:
            cursor.execute(
                """
                SELECT cluster_id, centroid_vector 
                FROM clustering_cache 
                ORDER BY cluster_id
            """
            )
            clusters = cursor.fetchall()

        if not clusters:
            logger.warning(
                "í´ëŸ¬ìŠ¤í„° ìºì‹œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì „ì²´ ì¬í´ëŸ¬ìŠ¤í„°ë§ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."
            )
            return process_batch_clustering(connection)

        # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        centroids = []
        for cluster in clusters:
            if isinstance(cluster["centroid_vector"], str):
                centroid = json.loads(cluster["centroid_vector"])
            else:
                centroid = cluster["centroid_vector"]
            centroids.append([float(x) for x in centroid])

        # ê°€ì¥ ìœ ì‚¬í•œ í´ëŸ¬ìŠ¤í„° ì°¾ê¸°
        similarities = []
        for centroid in centroids:
            similarity = cosine_similarity(emotion_vector, centroid)
            similarities.append(similarity)

        best_cluster = similarities.index(max(similarities))
        best_similarity = similarities[best_cluster]

        logger.info(
            f"ì‚¬ìš©ì {user_id}ì˜ ìµœì  í´ëŸ¬ìŠ¤í„°: {best_cluster} (ìœ ì‚¬ë„: {best_similarity:.3f})"
        )

        # ì»¤ë®¤ë‹ˆí‹° ì¶”ì²œ ì²˜ë¦¬
        process_user_community_recommendations(
            connection, user_id, emotion_vector, centroids, best_cluster
        )

        return {
            "user_id": user_id,
            "recommended_cluster": best_cluster,
            "similarity_score": float(best_similarity),
        }

    except Exception as e:
        logger.error(f"ê°œë³„ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        raise


def process_batch_clustering(connection):
    """ì „ì²´ ì¼ê¸° ë°ì´í„°ë¡œ ë°°ì¹˜ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰"""
    try:
        logger.info("ë°°ì¹˜ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘")

        # ì™„ë£Œëœ ëª¨ë“  ì¼ê¸° ì¡°íšŒ
        diaries = get_completed_diaries(connection)

        if len(diaries) < 3:
            logger.warning("í´ëŸ¬ìŠ¤í„°ë§ì„ ìœ„í•œ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {"message": "ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§ ë¶ˆê°€"}

        # ê°ì • ë²¡í„° íŒŒì‹±
        vectors, diary_ids, user_ids = parse_emotion_vectors(diaries)

        if len(vectors) < 3:
            logger.warning("ìœ íš¨í•œ ê°ì • ë²¡í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
            return {"message": "ìœ íš¨í•œ ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§ ë¶ˆê°€"}

        # í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
        labels, centroids = perform_clustering(vectors)

        # ê²°ê³¼ ì €ì¥
        save_clustering_results(connection, centroids)
        update_clustering_cache_member_counts(connection, labels, user_ids)

        # ê° ì‚¬ìš©ìë³„ ì»¤ë®¤ë‹ˆí‹° ì¶”ì²œ ì²˜ë¦¬
        processed_users = set()
        for i, (user_id, label) in enumerate(zip(user_ids, labels)):
            if user_id not in processed_users:
                user_vector = vectors[i]
                process_user_community_recommendations(
                    connection, user_id, user_vector, centroids, label
                )
                processed_users.add(user_id)

        logger.info(
            f"ë°°ì¹˜ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {len(centroids)}ê°œ í´ëŸ¬ìŠ¤í„°, {len(processed_users)}ëª… ì²˜ë¦¬"
        )

        publish_clustering_completion(
            total_diaries=len(diaries),
            total_users=len(processed_users), 
            clusters_created=len(centroids)
        )

        return {
            "message": "í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ",
            "clusters_created": len(centroids),
            "users_processed": len(processed_users),
        }

    except Exception as e:
        logger.error(f"ë°°ì¹˜ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {str(e)}")
        raise



def lambda_handler(event, context):
    """Lambda ë©”ì¸ í•¸ë“¤ëŸ¬ - API Gateway ì§€ì› ì¶”ê°€"""
    connection = None
    
    try:
        logger.info(f"Lambda ì‹¤í–‰ ì‹œì‘: {json.dumps(event)}")
        
        # DB ì—°ê²°
        connection = get_db_connection()
        
        # SQS ì´ë²¤íŠ¸ ì²˜ë¦¬ (ê°œë³„ ì¼ê¸° ë¶„ì„)
        if 'Records' in event:
            results = []
            
            for record in event['Records']:
                try:
                    # SQS ë©”ì‹œì§€ íŒŒì‹±
                    message_body = json.loads(record['body'])
                    
                    # SNS ë©”ì‹œì§€ì¸ ê²½ìš° Message í•„ë“œ íŒŒì‹±
                    if 'Message' in message_body:
                        message_data = json.loads(message_body['Message'])
                    else:
                        message_data = message_body
                    
                    # í•„ìˆ˜ ì •ë³´ ì¶”ì¶œ
                    diary_id = message_data.get('diaryId')
                    user_id = message_data.get('userId')
                    event_type = message_data.get('event')
                    
                    # ìœ íš¨ì„± ê²€ì‚¬
                    if event_type != 'diary_analyzed':
                        logger.warning(f"ì˜ëª»ëœ ì´ë²¤íŠ¸ íƒ€ì…: {event_type}")
                        continue
                    
                    if not diary_id or not user_id:
                        logger.warning(f"í•„ìˆ˜ ì •ë³´ ëˆ„ë½: diary_id={diary_id}, user_id={user_id}")
                        continue
                    
                    # DBì—ì„œ ê°ì • ë²¡í„° ì¡°íšŒ
                    with connection.cursor() as cursor:
                        cursor.execute("""
                            SELECT emotion_vector 
                            FROM diaries 
                            WHERE id = %s AND user_id = %s AND analysis_status = 'completed'
                        """, (diary_id, user_id))
                        
                        diary_result = cursor.fetchone()
                        
                        if not diary_result:
                            logger.warning(f"ì¼ê¸°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: diary_id={diary_id}")
                            continue
                        
                        # ê°ì • ë²¡í„° íŒŒì‹±
                        emotion_vector = diary_result['emotion_vector']
                        if isinstance(emotion_vector, str):
                            emotion_vector = json.loads(emotion_vector)
                        
                        if len(emotion_vector) != 10:
                            logger.warning(f"ì˜ëª»ëœ ê°ì • ë²¡í„° ê¸¸ì´: {len(emotion_vector)}")
                            continue
                        
                        emotion_vector = [float(x) for x in emotion_vector]
                    
                    # ê°œë³„ ë¶„ì„ ì²˜ë¦¬
                    result = process_individual_analysis(connection, diary_id, user_id, emotion_vector)
                    results.append(result)
                    
                except Exception as record_error:
                    logger.error(f"ë ˆì½”ë“œ ì²˜ë¦¬ ì˜¤ë¥˜: {str(record_error)}")
                    continue
            
            return {
                'statusCode': 200,
                'body': json.dumps({
                    'message': 'ê°œë³„ ë¶„ì„ ì™„ë£Œ',
                    'processed': len(results),
                    'results': results
                })
            }
        
        # EventBridge/CloudWatch Events (ë°°ì¹˜ ì²˜ë¦¬)
        elif event.get('source') == 'aws.events':
            result = process_batch_clustering(connection)
            
            return {
                'statusCode': 200,
                'body': json.dumps(result)
            }
        
        # API Gateway í˜¸ì¶œ ì²˜ë¦¬
        elif 'httpMethod' in event:
            http_method = event.get('httpMethod', '')
            resource_path = event.get('resource', '')
            path_parameters = event.get('pathParameters', {})
            
            logger.info(f"API Gateway í˜¸ì¶œ: {http_method} {resource_path}")
            
            # CORS í—¤ë” ì„¤ì •
            headers = {
                'Content-Type': 'application/json',
                'Access-Control-Allow-Origin': '*',
                'Access-Control-Allow-Headers': 'Content-Type',
                'Access-Control-Allow-Methods': 'GET,POST,OPTIONS'
            }
            
            # OPTIONS ë©”ì„œë“œ (CORS preflight)
            if http_method == 'OPTIONS':
                return {
                    'statusCode': 200,
                    'headers': headers,
                    'body': json.dumps({'message': 'CORS preflight'})
                }
            
            # GET /clustering/user/{userId} - ê°œë³„ ì‚¬ìš©ì ë¶„ì„
            if http_method == 'GET' and resource_path == '/clustering/user/{userId}':
                user_id = path_parameters.get('userId')
                
                if not user_id:
                    return {
                        'statusCode': 400,
                        'headers': headers,
                        'body': json.dumps({'error': 'userId íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤'})
                    }
                
                try:
                    user_id = int(user_id)
                except ValueError:
                    return {
                        'statusCode': 400,
                        'headers': headers,
                        'body': json.dumps({'error': 'ìœ íš¨í•˜ì§€ ì•Šì€ userIdì…ë‹ˆë‹¤'})
                    }
                
                # ì‚¬ìš©ìì˜ ìµœì‹  ì¼ê¸° ì¡°íšŒ
                with connection.cursor() as cursor:
                    cursor.execute("""
                        SELECT id, emotion_vector 
                        FROM diaries 
                        WHERE user_id = %s AND analysis_status = 'completed'
                        ORDER BY created_at DESC 
                        LIMIT 1
                    """, (user_id,))
                    
                    diary = cursor.fetchone()
                    if not diary:
                        return {
                            'statusCode': 404,
                            'headers': headers,
                            'body': json.dumps({'error': f'ì‚¬ìš©ì {user_id}ì˜ ë¶„ì„ëœ ì¼ê¸°ê°€ ì—†ìŠµë‹ˆë‹¤'})
                        }
                    
                    emotion_vector = diary['emotion_vector']
                    if isinstance(emotion_vector, str):
                        emotion_vector = json.loads(emotion_vector)
                    
                    emotion_vector = [float(x) for x in emotion_vector]
                
                result = process_individual_analysis(connection, diary['id'], user_id, emotion_vector)
                
                return {
                    'statusCode': 200,
                    'headers': headers,
                    'body': json.dumps({
                        'message': 'ê°œë³„ ì‚¬ìš©ì ë¶„ì„ ì™„ë£Œ',
                        'user_id': user_id,
                        'result': result
                    })
                }
            
            # POST /clustering/batch - ì „ì²´ ë°°ì¹˜ ë¶„ì„
            elif http_method == 'POST' and resource_path == '/clustering/batch':
                result = process_batch_clustering(connection)
                
                return {
                    'statusCode': 200,
                    'headers': headers,
                    'body': json.dumps({
                        'message': 'ë°°ì¹˜ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ',
                        'result': result
                    })
                }
            
            # ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²½ë¡œ
            else:
                return {
                    'statusCode': 404,
                    'headers': headers,
                    'body': json.dumps({'error': 'ì§€ì›í•˜ì§€ ì•ŠëŠ” API ê²½ë¡œì…ë‹ˆë‹¤'})
                }
        
        # ê¸°ì¡´ ë°©ì‹ (í•˜ìœ„ í˜¸í™˜ì„±)
        else:
            # ìš”ì²­ íƒ€ì…ì— ë”°ë¼ ë¶„ê¸°
            request_type = event.get('request_type', 'batch')
            
            if request_type == 'batch':
                result = process_batch_clustering(connection)
            else:
                # íŠ¹ì • ì‚¬ìš©ì ë¶„ì„
                user_id = event.get('user_id')
                if not user_id:
                    raise ValueError("user_idê°€ í•„ìš”í•©ë‹ˆë‹¤")
                
                # ì‚¬ìš©ìì˜ ìµœì‹  ì¼ê¸° ì¡°íšŒ
                with connection.cursor() as cursor:
                    cursor.execute("""
                        SELECT id, emotion_vector 
                        FROM diaries 
                        WHERE user_id = %s AND analysis_status = 'completed'
                        ORDER BY created_at DESC 
                        LIMIT 1
                    """, (user_id,))
                    
                    diary = cursor.fetchone()
                    if not diary:
                        raise ValueError(f"ì‚¬ìš©ì {user_id}ì˜ ë¶„ì„ëœ ì¼ê¸°ê°€ ì—†ìŠµë‹ˆë‹¤")
                    
                    emotion_vector = diary['emotion_vector']
                    if isinstance(emotion_vector, str):
                        emotion_vector = json.loads(emotion_vector)
                    
                    emotion_vector = [float(x) for x in emotion_vector]
                
                result = process_individual_analysis(connection, diary['id'], user_id, emotion_vector)
            
            return {
                'statusCode': 200,
                'body': json.dumps(result)
            }
    
    except Exception as e:
        logger.error(f"Lambda ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        
        # API Gateway ì˜¤ë¥˜ ì‘ë‹µ
        if 'httpMethod' in event:
            return {
                'statusCode': 500,
                'headers': {
                    'Content-Type': 'application/json',
                    'Access-Control-Allow-Origin': '*'
                },
                'body': json.dumps({'error': str(e)})
            }
        
        return {
            'statusCode': 500,
            'body': json.dumps({'error': str(e)})
        }
    
    finally:
        if connection:
            connection.close()


# í…ŒìŠ¤íŠ¸ìš© ë¡œì»¬ ì‹¤í–‰
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì´ë²¤íŠ¸
    test_event = {
        "Records": [
            {
                "body": json.dumps(
                    {
                        "Message": json.dumps(
                            {
                                "diaryId": 811,
                                "userId": 2,
                                "emotionVector": [
                                    0.1,
                                    0.7,
                                    0.0,
                                    0.3,
                                    0.0,
                                    0.0,
                                    0.2,
                                    0.0,
                                    0.1,
                                    0.0,
                                ],
                                "event": "diary_analyzed",
                            }
                        )
                    }
                )
            }
        ]
    }

    print("í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ê²°ê³¼:")
    result = lambda_handler(test_event, None)
    print(json.dumps(result, indent=2, ensure_ascii=False))
import json
import boto3
import pymysql
import os
import random
import math
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

sns_client = boto3.client("sns")

# í™˜ê²½ ë³€ìˆ˜
DB_HOST = os.environ["DB_HOST"]
DB_USER = os.environ["DB_USER"]
DB_PASSWORD = os.environ["DB_PASSWORD"]
DB_NAME = os.environ["DB_NAME"]
SNS_TOPIC_ARN = os.environ.get('SNS_TOPIC_ARN')

def get_db_connection():
    return pymysql.connect(
        host=DB_HOST,
        user=DB_USER,
        password=DB_PASSWORD,
        database=DB_NAME,
        charset="utf8mb4",
        cursorclass=pymysql.cursors.DictCursor,
    )

def publish_clustering_completion(total_diaries, total_users, clusters_created):
    try:
        if not SNS_TOPIC_ARN:
            logger.warning("SNS_TOPIC_ARN í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
            
        current_time = datetime.now()
        message = {
            "totalDiaries": total_diaries,
            "totalUsers": total_users,
            "clustersCreated": clusters_created,
            "event": "batch_clustering_completed",
            "completedAt": current_time.isoformat(),
            "completedAtKST": (current_time + timedelta(hours=9)).strftime('%Y-%m-%d %H:%M:%S')
        }
        
        sns_client.publish(
            TopicArn=SNS_TOPIC_ARN,
            Message=json.dumps(message),
            Subject='batch_clustering_completed'
        )
        logger.info(f"í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ SNS ë©”ì‹œì§€ ë°œí–‰: {total_users}ëª… ì²˜ë¦¬ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"SNS ë°œí–‰ ì˜¤ë¥˜: {str(e)}")

def cosine_similarity(a, b):
    dot_product = sum(x * y for x, y in zip(a, b))
    norm_a = math.sqrt(sum(x * x for x in a))
    norm_b = math.sqrt(sum(x * x for x in b))

    if norm_a == 0 or norm_b == 0:
        return 0.1

    return dot_product / (norm_a * norm_b)

def euclidean_distance(a, b):
    return math.sqrt(sum((x - y) ** 2 for x, y in zip(a, b)))

def simple_kmeans_python(vectors, n_clusters, max_iters=100, random_seed=42):
    n_samples = len(vectors)
    n_features = len(vectors[0]) if vectors else 0

    if n_samples < n_clusters:
        labels = list(range(n_samples))
        centroids = [list(vector) for vector in vectors]
        return labels, centroids

    random.seed(random_seed)

    centroids = []
    centroids.append(list(vectors[random.randint(0, n_samples - 1)]))

    for _ in range(1, n_clusters):
        distances = []
        for vector in vectors:
            min_dist = min(
                euclidean_distance(vector, centroid) ** 2 for centroid in centroids
            )
            distances.append(min_dist)

        total_dist = sum(distances)
        if total_dist == 0:
            centroids.append(list(vectors[random.randint(0, n_samples - 1)]))
            continue

        probabilities = [d / total_dist for d in distances]
        cumulative_probs = []
        cumsum = 0
        for p in probabilities:
            cumsum += p
            cumulative_probs.append(cumsum)

        r = random.random()
        for j, p in enumerate(cumulative_probs):
            if r < p:
                centroids.append(list(vectors[j]))
                break

    for iteration in range(max_iters):
        labels = []
        for vector in vectors:
            distances = [euclidean_distance(vector, centroid) for centroid in centroids]
            closest_cluster = distances.index(min(distances))
            labels.append(closest_cluster)

        new_centroids = []
        for k in range(n_clusters):
            cluster_points = [vectors[i] for i in range(n_samples) if labels[i] == k]

            if cluster_points:
                new_centroid = []
                for feature_idx in range(n_features):
                    feature_sum = sum(point[feature_idx] for point in cluster_points)
                    new_centroid.append(feature_sum / len(cluster_points))
                new_centroids.append(new_centroid)
            else:
                new_centroids.append(centroids[k][:])

        converged = True
        for old, new in zip(centroids, new_centroids):
            for old_val, new_val in zip(old, new):
                if abs(old_val - new_val) > 1e-4:
                    converged = False
                    break
            if not converged:
                break

        if converged:
            logger.info(f"K-means ìˆ˜ë ´: {iteration + 1}ë²ˆì§¸ ë°˜ë³µ")
            break

        centroids = new_centroids

    return labels, centroids

def get_completed_diaries(connection) -> List[Dict]:
    with connection.cursor() as cursor:
        query = """
        SELECT 
            id,
            user_id,
            emotion_vector,
            primary_emotion,
            created_at
        FROM diaries 
        WHERE analysis_status = 'completed' 
            AND emotion_vector IS NOT NULL
            AND JSON_LENGTH(emotion_vector) = 10
        ORDER BY created_at DESC
        """
        cursor.execute(query)
        return cursor.fetchall()

def parse_emotion_vectors(diaries: List[Dict]) -> Tuple[List[List[float]], List[int], List[int]]:
    vectors = []
    diary_ids = []
    user_ids = []

    for diary in diaries:
        try:
            if isinstance(diary["emotion_vector"], str):
                emotion_vector = json.loads(diary["emotion_vector"])
            else:
                emotion_vector = diary["emotion_vector"]

            if len(emotion_vector) == 10:
                vectors.append([float(x) for x in emotion_vector])
                diary_ids.append(diary["id"])
                user_ids.append(diary["user_id"])
            else:
                logger.warning(f"ì¼ê¸° {diary['id']}: ì˜ëª»ëœ ë²¡í„° ê¸¸ì´ {len(emotion_vector)}")

        except (json.JSONDecodeError, TypeError, ValueError) as e:
            logger.warning(f"ì¼ê¸° {diary['id']}: ë²¡í„° íŒŒì‹± ì‹¤íŒ¨ - {str(e)}")

    return vectors, diary_ids, user_ids

def perform_clustering(vectors: List[List[float]], min_cluster_size: int = 3) -> Tuple[List[int], List[List[float]]]:
    n_samples = len(vectors)

    if n_samples < min_cluster_size:
        logger.warning(f"ìƒ˜í”Œ ìˆ˜ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤: {n_samples}")
        labels = [0] * n_samples
        if vectors:
            n_features = len(vectors[0])
            centroid = [
                sum(vectors[i][j] for i in range(n_samples)) / n_samples
                for j in range(n_features)
            ]
            centroids = [centroid]
        else:
            centroids = [[0.0] * 10]
        return labels, centroids

    if n_samples < 10:
        n_clusters = 2
    elif n_samples < 50:
        n_clusters = min(5, n_samples // 3)
    else:
        n_clusters = min(10, n_samples // 10)

    logger.info(f"í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰: {n_samples}ê°œ ìƒ˜í”Œ, {n_clusters}ê°œ í´ëŸ¬ìŠ¤í„°")

    labels, centroids = simple_kmeans_python(
        vectors=vectors, n_clusters=n_clusters, max_iters=300, random_seed=42
    )

    return labels, centroids

def get_cluster_statistics(labels: List[int], user_ids: List[int], diary_ids: List[int]) -> Dict:
    cluster_stats = {}
    
    for i, (label, user_id, diary_id) in enumerate(zip(labels, user_ids, diary_ids)):
        if label not in cluster_stats:
            cluster_stats[label] = {
                'diary_count': 0,
                'unique_users': set(),
                'diary_ids': []
            }
        
        cluster_stats[label]['diary_count'] += 1
        cluster_stats[label]['unique_users'].add(user_id)
        cluster_stats[label]['diary_ids'].append(diary_id)
    
    for cluster_id in cluster_stats:
        cluster_stats[cluster_id]['unique_user_count'] = len(cluster_stats[cluster_id]['unique_users'])
        cluster_stats[cluster_id]['unique_users'] = list(cluster_stats[cluster_id]['unique_users'])
    
    return cluster_stats

def save_clustering_results(connection, centroids: List[List[float]]) -> Dict[int, int]:
    cluster_id_mapping = {}

    with connection.cursor() as cursor:
        cursor.execute("DELETE FROM clustering_cache")
        deleted_clusters = cursor.rowcount

        for i, centroid in enumerate(centroids):
            centroid_json = json.dumps(centroid)

            cursor.execute(
                "INSERT INTO clustering_cache (cluster_id, centroid_vector, member_count, created_at) VALUES (%s, %s, %s, NOW())",
                (i, centroid_json, 0)
            )

            cluster_id_mapping[i] = cursor.lastrowid

    connection.commit()
    logger.info(f"í´ëŸ¬ìŠ¤í„° ìºì‹œ ì—…ë°ì´íŠ¸: {deleted_clusters}ê°œ ì‚­ì œ, {len(centroids)}ê°œ ìƒì„±")
    return cluster_id_mapping

def get_user_current_communities(connection, user_id: int) -> List[int]:
    """ì‚¬ìš©ìê°€ í˜„ì¬ ê°€ì…í•œ ëª¨ë“  ì»¤ë®¤ë‹ˆí‹° ID ëª©ë¡ ì¡°íšŒ"""
    with connection.cursor() as cursor:
        cursor.execute(
            "SELECT community_id FROM community_members WHERE user_id = %s AND is_active = 1",
            (user_id,)
        )
        return [row["community_id"] for row in cursor.fetchall()]

def get_or_create_community(connection, cluster_id: int, emotion_theme: str = None) -> int:
    """í´ëŸ¬ìŠ¤í„°ì— í•´ë‹¹í•˜ëŠ” ì»¤ë®¤ë‹ˆí‹° ì¡°íšŒ ë˜ëŠ” ìƒì„±"""
    with connection.cursor() as cursor:
        # ê¸°ì¡´ ì»¤ë®¤ë‹ˆí‹° ì¡°íšŒ
        cursor.execute(
            "SELECT id FROM communities WHERE name LIKE %s AND is_default = 1 ORDER BY created_at DESC LIMIT 1",
            (f"ê°ì •ë‚˜ëˆ”ë°© #{cluster_id}%",)
        )

        result = cursor.fetchone()
        if result:
            return result["id"]

        # ìƒˆ ì»¤ë®¤ë‹ˆí‹° ìƒì„±
        community_name = f"ê°ì •ë‚˜ëˆ”ë°© #{cluster_id}"
        description = f"í´ëŸ¬ìŠ¤í„° {cluster_id}ì˜ ë¹„ìŠ·í•œ ê°ì •ì„ ê°€ì§„ ì‚¬ëŒë“¤ì˜ ë‚˜ëˆ”ë°©"

        cursor.execute(
            "INSERT INTO communities (name, description, creator_id, is_default, emotion_theme, created_at) VALUES (%s, %s, NULL, 1, %s, NOW())",
            (community_name, description, emotion_theme)
        )

        community_id = cursor.lastrowid
        connection.commit()

        logger.info(f"ìƒˆ ì»¤ë®¤ë‹ˆí‹° ìƒì„±: {community_name} (ID: {community_id})")
        return community_id

def join_user_to_community(connection, user_id: int, community_id: int) -> bool:
    """ì‚¬ìš©ìë¥¼ ì»¤ë®¤ë‹ˆí‹°ì— ê°€ì…ì‹œí‚¤ê¸° (ì¤‘ë³µ ê°€ì… ë°©ì§€)"""
    try:
        with connection.cursor() as cursor:
            # ì´ë¯¸ ê°€ì…ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            cursor.execute(
                "SELECT id FROM community_members WHERE user_id = %s AND community_id = %s",
                (user_id, community_id)
            )
            
            existing_member = cursor.fetchone()
            
            if existing_member:
                # ì´ë¯¸ ê°€ì…ë˜ì–´ ìˆë‹¤ë©´ is_activeë¥¼ 1ë¡œ ì—…ë°ì´íŠ¸
                cursor.execute(
                    "UPDATE community_members SET is_active = 1, joined_at = NOW() WHERE user_id = %s AND community_id = %s",
                    (user_id, community_id)
                )
                logger.info(f"ì‚¬ìš©ì {user_id}ì˜ ì»¤ë®¤ë‹ˆí‹° {community_id} ì¬í™œì„±í™”")
            else:
                # ìƒˆë¡œ ê°€ì…
                cursor.execute(
                    "INSERT INTO community_members (user_id, community_id, joined_at, is_active) VALUES (%s, %s, NOW(), 1)",
                    (user_id, community_id)
                )
                logger.info(f"ì‚¬ìš©ì {user_id}ë¥¼ ì»¤ë®¤ë‹ˆí‹° {community_id}ì— ìƒˆë¡œ ê°€ì…")
            
            connection.commit()
            return True
            
    except Exception as e:
        logger.error(f"ì‚¬ìš©ì {user_id}ì˜ ì»¤ë®¤ë‹ˆí‹° {community_id} ê°€ì… ì‹¤íŒ¨: {str(e)}")
        connection.rollback()
        return False

def process_user_community_assignment(
    connection,
    user_id: int,
    user_vector: List[float],
    centroids: List[List[float]],
    current_cluster: int,
) -> Dict:
    """ì‚¬ìš©ìë¥¼ ì ì ˆí•œ ì»¤ë®¤ë‹ˆí‹°ì— ìë™ ê°€ì…ì‹œí‚¤ê¸°"""
    assignment_result = {
        'user_id': user_id,
        'current_cluster': current_cluster,
        'joined_communities': [],
        'skipped_communities': [],
        'total_assignments': 0
    }
    
    try:
        # í˜„ì¬ ê°€ì…í•œ ì»¤ë®¤ë‹ˆí‹° ëª©ë¡
        current_communities = get_user_current_communities(connection, user_id)
        
        # ëª¨ë“  í´ëŸ¬ìŠ¤í„°ì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = []
        for centroid in centroids:
            similarity = cosine_similarity(user_vector, centroid)
            similarities.append(similarity)

        # ìœ ì‚¬ë„ ë†’ì€ ìˆœìœ¼ë¡œ í´ëŸ¬ìŠ¤í„° ì •ë ¬
        cluster_scores = [(i, score) for i, score in enumerate(similarities)]
        cluster_scores.sort(key=lambda x: x[1], reverse=True)

        assignments_made = 0
        max_assignments = 3  # ìµœëŒ€ 3ê°œ ì»¤ë®¤ë‹ˆí‹°ê¹Œì§€ ìë™ ê°€ì…
        
        for cluster_id, similarity_score in cluster_scores:
            if assignments_made >= max_assignments:
                break
                
            # ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’ (20% ì´ìƒ)
            if similarity_score < 0.2:
                assignment_result['skipped_communities'].append({
                    'cluster_id': cluster_id,
                    'reason': f'ìœ ì‚¬ë„ ë„ˆë¬´ ë‚®ìŒ ({similarity_score:.3f})',
                    'similarity_score': float(similarity_score)
                })
                continue

            # í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì˜ ì»¤ë®¤ë‹ˆí‹° ì¡°íšŒ/ìƒì„±
            community_id = get_or_create_community(connection, cluster_id)

            # ì´ë¯¸ ê°€ì…ëœ ì»¤ë®¤ë‹ˆí‹°ë©´ ìŠ¤í‚µ
            if community_id in current_communities:
                assignment_result['skipped_communities'].append({
                    'cluster_id': cluster_id,
                    'community_id': community_id,
                    'reason': 'ì´ë¯¸ ê°€ì…ëœ ì»¤ë®¤ë‹ˆí‹°',
                    'similarity_score': float(similarity_score)
                })
                continue

            # ì»¤ë®¤ë‹ˆí‹°ì— ê°€ì…
            if join_user_to_community(connection, user_id, community_id):
                assignment_result['joined_communities'].append({
                    'cluster_id': cluster_id,
                    'community_id': community_id,
                    'similarity_score': float(similarity_score),
                    'assignment_reason': get_assignment_reason(cluster_id, current_cluster, similarity_score)
                })
                assignments_made += 1
            else:
                assignment_result['skipped_communities'].append({
                    'cluster_id': cluster_id,
                    'community_id': community_id,
                    'reason': 'ê°€ì… ì²˜ë¦¬ ì‹¤íŒ¨',
                    'similarity_score': float(similarity_score)
                })

        assignment_result['total_assignments'] = assignments_made
        logger.info(f"ì‚¬ìš©ì {user_id}: {assignments_made}ê°œ ì»¤ë®¤ë‹ˆí‹°ì— ìë™ ê°€ì… ì™„ë£Œ")

    except Exception as e:
        logger.error(f"ì‚¬ìš©ì {user_id} ì»¤ë®¤ë‹ˆí‹° í• ë‹¹ ì‹¤íŒ¨: {str(e)}")
        assignment_result['error'] = str(e)
    
    return assignment_result

def get_assignment_reason(cluster_id: int, current_cluster: int, similarity_score: float) -> str:
    """ê°€ì… ì´ìœ  ìƒì„±"""
    if cluster_id == current_cluster:
        return "í˜„ì¬ ê°ì •ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ì»¤ë®¤ë‹ˆí‹°"
    elif similarity_score > 0.8:
        return "ê°ì • íŒ¨í„´ì´ ë§¤ìš° ìœ ì‚¬í•¨"
    elif similarity_score > 0.6:
        return "ë¹„ìŠ·í•œ ê°ì •ì„ ìì£¼ ê²½í—˜í•¨"
    elif similarity_score > 0.4:
        return "ê´€ì‹¬ ìˆì„ ë§Œí•œ ê°ì • ì£¼ì œ"
    else:
        return "ìƒˆë¡œìš´ ê°ì • ê²½í—˜ì„ ë‚˜ëˆŒ ìˆ˜ ìˆëŠ” ì»¤ë®¤ë‹ˆí‹°"

def update_clustering_cache_member_counts(connection, labels: List[int], user_ids: List[int]):
    """í´ëŸ¬ìŠ¤í„°ë³„ ë©¤ë²„ ìˆ˜ ì—…ë°ì´íŠ¸"""
    cluster_counts = {}
    for label in labels:
        cluster_counts[label] = cluster_counts.get(label, 0) + 1

    with connection.cursor() as cursor:
        for cluster_id, count in cluster_counts.items():
            cursor.execute(
                "UPDATE clustering_cache SET member_count = %s WHERE cluster_id = %s",
                (count, cluster_id)
            )

    connection.commit()
    logger.info(f"í´ëŸ¬ìŠ¤í„° ë©¤ë²„ ìˆ˜ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(cluster_counts)}ê°œ í´ëŸ¬ìŠ¤í„°")

def process_individual_analysis(connection, diary_id: int, user_id: int, emotion_vector: List[float]) -> Dict:
    """ê°œë³„ ì¼ê¸° ë¶„ì„ ë° ì»¤ë®¤ë‹ˆí‹° ìë™ ê°€ì… ì²˜ë¦¬"""
    try:
        logger.info(f"ê°œë³„ ì¼ê¸° ë¶„ì„ ì‹œì‘: diary_id={diary_id}, user_id={user_id}")

        # í˜„ì¬ í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì ë“¤ ì¡°íšŒ
        with connection.cursor() as cursor:
            cursor.execute(
                "SELECT cluster_id, centroid_vector FROM clustering_cache ORDER BY cluster_id"
            )
            clusters = cursor.fetchall()

        if not clusters:
            logger.warning("í´ëŸ¬ìŠ¤í„° ìºì‹œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì „ì²´ ì¬í´ëŸ¬ìŠ¤í„°ë§ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.")
            return process_batch_clustering(connection)

        # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        centroids = []
        for cluster in clusters:
            if isinstance(cluster["centroid_vector"], str):
                centroid = json.loads(cluster["centroid_vector"])
            else:
                centroid = cluster["centroid_vector"]
            centroids.append([float(x) for x in centroid])

        # ê°€ì¥ ìœ ì‚¬í•œ í´ëŸ¬ìŠ¤í„° ì°¾ê¸°
        similarities = []
        for centroid in centroids:
            similarity = cosine_similarity(emotion_vector, centroid)
            similarities.append(similarity)

        best_cluster = similarities.index(max(similarities))
        best_similarity = similarities[best_cluster]

        logger.info(f"ì‚¬ìš©ì {user_id}ì˜ ìµœì  í´ëŸ¬ìŠ¤í„°: {best_cluster} (ìœ ì‚¬ë„: {best_similarity:.3f})")

        # ì»¤ë®¤ë‹ˆí‹° ìë™ ê°€ì… ì²˜ë¦¬
        assignment_result = process_user_community_assignment(
            connection, user_id, emotion_vector, centroids, best_cluster
        )

        result = {
            "diary_id": diary_id,
            "user_id": user_id,
            "assigned_cluster": best_cluster,
            "similarity_score": float(best_similarity),
            "community_assignment": assignment_result
        }

        # SNS ë©”ì‹œì§€ ë°œí–‰
        try:
            if SNS_TOPIC_ARN:
                # ê°€ì…ëœ ì»¤ë®¤ë‹ˆí‹° ì •ë³´ í¬ë§·íŒ…
                joined_communities_info = []
                for community in assignment_result.get('joined_communities', []):
                    joined_communities_info.append({
                        "community_id": community['community_id'],
                        "cluster_id": community['cluster_id'],
                        "similarity_score": community['similarity_score'],
                        "reason": community['assignment_reason']
                    })
                
                message = {
                    "user_id": user_id,
                    "diary_id": diary_id,
                    "assigned_cluster": best_cluster,
                    "similarity_score": float(best_similarity),
                    "communities_joined_count": len(assignment_result.get('joined_communities', [])),
                    "communities_joined": joined_communities_info,
                    "event": "user_communities_assigned",
                    "completedAt": datetime.now().isoformat()
                }
                
                sns_client.publish(
                    TopicArn=SNS_TOPIC_ARN,
                    Message=json.dumps(message),
                    Subject='user_communities_assigned'
                )
                logger.info(f"ì‚¬ìš©ì ì»¤ë®¤ë‹ˆí‹° ê°€ì… ì™„ë£Œ SNS ë°œí–‰: user_id={user_id}, ê°€ì…ëœ ì»¤ë®¤ë‹ˆí‹° {len(joined_communities_info)}ê°œ")
        except Exception as sns_error:
            logger.error(f"SNS ë°œí–‰ ì‹¤íŒ¨: {str(sns_error)}")

        return result

    except Exception as e:
        logger.error(f"ê°œë³„ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        return {
            "diary_id": diary_id,
            "user_id": user_id,
            "error": str(e),
            "status": "failed"
        }

def process_batch_clustering(connection) -> Dict:
    """ì „ì²´ ì¼ê¸° ë°ì´í„°ë¡œ ë°°ì¹˜ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ ë° ì»¤ë®¤ë‹ˆí‹° ìë™ ê°€ì…"""
    try:
        start_time = datetime.now()
        logger.info("ë°°ì¹˜ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘")

        # ì™„ë£Œëœ ëª¨ë“  ì¼ê¸° ì¡°íšŒ
        diaries = get_completed_diaries(connection)

        if len(diaries) < 3:
            logger.warning("í´ëŸ¬ìŠ¤í„°ë§ì„ ìœ„í•œ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {
                "status": "insufficient_data",
                "message": "ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§ ë¶ˆê°€",
                "total_diaries": len(diaries),
                "processing_time": 0
            }

        # ê°ì • ë²¡í„° íŒŒì‹±
        vectors, diary_ids, user_ids = parse_emotion_vectors(diaries)

        if len(vectors) < 3:
            logger.warning("ìœ íš¨í•œ ê°ì • ë²¡í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
            return {
                "status": "insufficient_valid_data",
                "message": "ìœ íš¨í•œ ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§ ë¶ˆê°€",
                "total_diaries": len(diaries),
                "valid_vectors": len(vectors),
                "processing_time": 0
            }

        # í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
        labels, centroids = perform_clustering(vectors)
        cluster_stats = get_cluster_statistics(labels, user_ids, diary_ids)
        
        # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì €ì¥
        save_clustering_results(connection, centroids)
        update_clustering_cache_member_counts(connection, labels, user_ids)

        # ê° ì‚¬ìš©ìë³„ ì»¤ë®¤ë‹ˆí‹° ìë™ ê°€ì… ì²˜ë¦¬
        processed_users = set()
        user_assignments = []
        total_assignments = 0
        
        for i, (user_id, label) in enumerate(zip(user_ids, labels)):
            if user_id not in processed_users:
                user_vector = vectors[i]
                assignment_result = process_user_community_assignment(
                    connection, user_id, user_vector, centroids, label
                )
                user_assignments.append(assignment_result)
                total_assignments += assignment_result.get('total_assignments', 0)
                processed_users.add(user_id)

        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()

        # SNS ì™„ë£Œ ë©”ì‹œì§€ ë°œí–‰
        publish_clustering_completion(
            total_diaries=len(diaries),
            total_users=len(processed_users), 
            clusters_created=len(centroids)
        )

        # ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ ì¶”ê°€ SNS ë©”ì‹œì§€
        try:
            if SNS_TOPIC_ARN:
                batch_message = {
                    "total_diaries": len(diaries),
                    "total_users": len(processed_users),
                    "clusters_created": len(centroids),
                    "total_community_assignments": total_assignments,
                    "users_with_new_communities": len([u for u in user_assignments if u.get('total_assignments', 0) > 0]),
                    "event": "batch_community_assignment_completed",
                    "completedAt": end_time.isoformat(),
                    "processing_time_seconds": processing_time
                }
                
                sns_client.publish(
                    TopicArn=SNS_TOPIC_ARN,
                    Message=json.dumps(batch_message),
                    Subject='batch_community_assignment_completed'
                )
                logger.info(f"ë°°ì¹˜ ì»¤ë®¤ë‹ˆí‹° ê°€ì… ì™„ë£Œ SNS ë°œí–‰: {total_assignments}ê±´ ê°€ì… ì²˜ë¦¬")
        except Exception as sns_error:
            logger.error(f"ë°°ì¹˜ ì™„ë£Œ SNS ë°œí–‰ ì‹¤íŒ¨: {str(sns_error)}")

        result = {
            "status": "success",
            "message": "ë°°ì¹˜ í´ëŸ¬ìŠ¤í„°ë§ ë° ì»¤ë®¤ë‹ˆí‹° ìë™ ê°€ì… ì™„ë£Œ",
            "processing_time": processing_time,
            "total_diaries": len(diaries),
            "valid_vectors": len(vectors),
            "clusters_created": len(centroids),
            "users_processed": len(processed_users),
            "total_community_assignments": total_assignments,
            "cluster_statistics": cluster_stats,
            "user_assignments": user_assignments,
            "timestamp": end_time.isoformat()
        }

        logger.info(f"ë°°ì¹˜ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {len(centroids)}ê°œ í´ëŸ¬ìŠ¤í„°, {len(processed_users)}ëª… ì²˜ë¦¬, {total_assignments}ê±´ ì»¤ë®¤ë‹ˆí‹° ê°€ì…, {processing_time:.2f}ì´ˆ ì†Œìš”")

        return result

    except Exception as e:
        logger.error(f"ë°°ì¹˜ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {str(e)}")
        return {
            "status": "error",
            "message": f"ë°°ì¹˜ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }

def lambda_handler(event, context):
    connection = None
    
    try:
        logger.info(f"Lambda ì‹¤í–‰ ì‹œì‘: {json.dumps(event)}")
        
        connection = get_db_connection()
        
        # SQS ì´ë²¤íŠ¸ ì²˜ë¦¬ (ê°œë³„ ì¼ê¸° ë¶„ì„)
        if 'Records' in event:
            results = []
            
            for record in event['Records']:
                try:
                    message_body = json.loads(record['body'])
                    
                    # SNS ë©”ì‹œì§€ì¸ ê²½ìš° Message í•„ë“œ íŒŒì‹±
                    if 'Message' in message_body:
                        message_data = json.loads(message_body['Message'])
                    else:
                        message_data = message_body
                    
                    # í•„ìˆ˜ ì •ë³´ ì¶”ì¶œ
                    diary_id = message_data.get('diaryId')
                    user_id = message_data.get('userId')
                    event_type = message_data.get('event')
                    
                    # ìœ íš¨ì„± ê²€ì‚¬
                    if event_type != 'diary_analyzed':
                        logger.warning(f"ì˜ëª»ëœ ì´ë²¤íŠ¸ íƒ€ì…: {event_type}")
                        continue
                    
                    if not diary_id or not user_id:
                        logger.warning(f"í•„ìˆ˜ ì •ë³´ ëˆ„ë½: diary_id={diary_id}, user_id={user_id}")
                        continue
                    
                    # DBì—ì„œ ê°ì • ë²¡í„° ì¡°íšŒ
                    with connection.cursor() as cursor:
                        cursor.execute(
                            "SELECT emotion_vector FROM diaries WHERE id = %s AND user_id = %s AND analysis_status = 'completed'",
                            (diary_id, user_id)
                        )
                        
                        diary_result = cursor.fetchone()
                        
                        if not diary_result:
                            logger.warning(f"ì¼ê¸°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: diary_id={diary_id}")
                            continue
                        
                        # ê°ì • ë²¡í„° íŒŒì‹±
                        emotion_vector = diary_result['emotion_vector']
                        if isinstance(emotion_vector, str):
                            emotion_vector = json.loads(emotion_vector)
                        
                        if len(emotion_vector) != 10:
                            logger.warning(f"ì˜ëª»ëœ ê°ì • ë²¡í„° ê¸¸ì´: {len(emotion_vector)}")
                            continue
                        
                        emotion_vector = [float(x) for x in emotion_vector]
                    
                    # ê°œë³„ ë¶„ì„ ë° ì»¤ë®¤ë‹ˆí‹° ê°€ì… ì²˜ë¦¬
                    result = process_individual_analysis(connection, diary_id, user_id, emotion_vector)
                    results.append(result)
                    
                except Exception as record_error:
                    logger.error(f"ë ˆì½”ë“œ ì²˜ë¦¬ ì˜¤ë¥˜: {str(record_error)}")
                    results.append({
                        "error": str(record_error),
                        "status": "failed"
                    })
                    continue
            
            return {
                'statusCode': 200,
                'body': json.dumps({
                    'message': 'ê°œë³„ ë¶„ì„ ë° ì»¤ë®¤ë‹ˆí‹° ê°€ì… ì™„ë£Œ',
                    'processed_count': len([r for r in results if 'error' not in r]),
                    'failed_count': len([r for r in results if 'error' in r]),
                    'total_records': len(event['Records']),
                    'results': results,
                    'timestamp': datetime.now().isoformat()
                }, ensure_ascii=False)
            }
        
        # EventBridge/CloudWatch Events (ë°°ì¹˜ ì²˜ë¦¬)
        elif event.get('source') == 'aws.events':
            result = process_batch_clustering(connection)
            
            return {
                'statusCode': 200,
                'body': json.dumps(result, ensure_ascii=False)
            }
        
        # API Gateway í˜¸ì¶œ ì²˜ë¦¬
        elif 'httpMethod' in event:
            http_method = event.get('httpMethod', '')
            resource_path = event.get('resource', '')
            path_parameters = event.get('pathParameters', {})
            
            logger.info(f"API Gateway í˜¸ì¶œ: {http_method} {resource_path}")
            
            # CORS í—¤ë” ì„¤ì •
            headers = {
                'Content-Type': 'application/json',
                'Access-Control-Allow-Origin': '*',
                'Access-Control-Allow-Headers': 'Content-Type',
                'Access-Control-Allow-Methods': 'GET,OPTIONS'
            }
            
            # OPTIONS ë©”ì„œë“œ (CORS preflight)
            if http_method == 'OPTIONS':
                return {
                    'statusCode': 200,
                    'headers': headers,
                    'body': json.dumps({'message': 'CORS preflight'})
                }
            
            # GET /api/community/cluster/{userId} - ê°œë³„ ì‚¬ìš©ì ë¶„ì„ ë° ì»¤ë®¤ë‹ˆí‹° ê°€ì…
            if http_method == 'GET' and resource_path == '/api/community/cluster/{userId}':
                user_id = path_parameters.get('userId')
                
                if not user_id:
                    return {
                        'statusCode': 400,
                        'headers': headers,
                        'body': json.dumps({'error': 'userId íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤'}, ensure_ascii=False)
                    }
                
                try:
                    user_id = int(user_id)
                except ValueError:
                    return {
                        'statusCode': 400,
                        'headers': headers,
                        'body': json.dumps({'error': 'ìœ íš¨í•˜ì§€ ì•Šì€ userIdì…ë‹ˆë‹¤'}, ensure_ascii=False)
                    }
                
                # ì‚¬ìš©ìì˜ ìµœì‹  ì¼ê¸° ì¡°íšŒ
                with connection.cursor() as cursor:
                    cursor.execute(
                        "SELECT id, emotion_vector FROM diaries WHERE user_id = %s AND analysis_status = 'completed' ORDER BY created_at DESC LIMIT 1",
                        (user_id,)
                    )
                    
                    diary = cursor.fetchone()
                    if not diary:
                        return {
                            'statusCode': 404,
                            'headers': headers,
                            'body': json.dumps({
                                'error': f'ì‚¬ìš©ì {user_id}ì˜ ë¶„ì„ëœ ì¼ê¸°ê°€ ì—†ìŠµë‹ˆë‹¤',
                                'user_id': user_id
                            }, ensure_ascii=False)
                        }
                    
                    emotion_vector = diary['emotion_vector']
                    if isinstance(emotion_vector, str):
                        emotion_vector = json.loads(emotion_vector)
                    
                    emotion_vector = [float(x) for x in emotion_vector]
                
                # ê°œë³„ ë¶„ì„ ë° ì»¤ë®¤ë‹ˆí‹° ê°€ì… ì²˜ë¦¬
                result = process_individual_analysis(connection, diary['id'], user_id, emotion_vector)
                
                return {
                    'statusCode': 200,
                    'headers': headers,
                    'body': json.dumps({
                        'message': 'ê°œë³„ ì‚¬ìš©ì ë¶„ì„ ë° ì»¤ë®¤ë‹ˆí‹° ê°€ì… ì™„ë£Œ',
                        'user_id': user_id,
                        'result': result,
                        'timestamp': datetime.now().isoformat()
                    }, ensure_ascii=False)
                }
            
            # ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²½ë¡œ
            else:
                return {
                    'statusCode': 404,
                    'headers': headers,
                    'body': json.dumps({
                        'error': 'ì§€ì›í•˜ì§€ ì•ŠëŠ” API ê²½ë¡œì…ë‹ˆë‹¤',
                        'method': http_method,
                        'path': resource_path,
                        'available_endpoints': [
                            'GET /api/community/cluster/{userId}'
                        ]
                    }, ensure_ascii=False)
                }
        
        # ê¸°ì¡´ ë°©ì‹ (í•˜ìœ„ í˜¸í™˜ì„±)
        else:
            request_type = event.get('request_type', 'batch')
            
            if request_type == 'batch':
                result = process_batch_clustering(connection)
                return {
                    'statusCode': 200,
                    'body': json.dumps(result, ensure_ascii=False)
                }
            else:
                # íŠ¹ì • ì‚¬ìš©ì ë¶„ì„
                user_id = event.get('user_id')
                if not user_id:
                    return {
                        'statusCode': 400,
                        'body': json.dumps({
                            'error': 'user_idê°€ í•„ìš”í•©ë‹ˆë‹¤',
                            'event': event
                        }, ensure_ascii=False)
                    }
                
                # ì‚¬ìš©ìì˜ ìµœì‹  ì¼ê¸° ì¡°íšŒ
                with connection.cursor() as cursor:
                    cursor.execute(
                        "SELECT id, emotion_vector FROM diaries WHERE user_id = %s AND analysis_status = 'completed' ORDER BY created_at DESC LIMIT 1",
                        (user_id,)
                    )
                    
                    diary = cursor.fetchone()
                    if not diary:
                        return {
                            'statusCode': 404,
                            'body': json.dumps({
                                'error': f'ì‚¬ìš©ì {user_id}ì˜ ë¶„ì„ëœ ì¼ê¸°ê°€ ì—†ìŠµë‹ˆë‹¤'
                            }, ensure_ascii=False)
                        }
                    
                    emotion_vector = diary['emotion_vector']
                    if isinstance(emotion_vector, str):
                        emotion_vector = json.loads(emotion_vector)
                    
                    emotion_vector = [float(x) for x in emotion_vector]
                
                # ê°œë³„ ë¶„ì„ ë° ì»¤ë®¤ë‹ˆí‹° ê°€ì… ì²˜ë¦¬
                result = process_individual_analysis(connection, diary['id'], user_id, emotion_vector)
                
                return {
                    'statusCode': 200,
                    'body': json.dumps(result, ensure_ascii=False)
                }
    
    except Exception as e:
        logger.error(f"Lambda ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        
        # ì˜¤ë¥˜ ì‘ë‹µ êµ¬ì„±
        error_response = {
            'error': str(e),
            'timestamp': datetime.now().isoformat(),
            'event_type': 'unknown'
        }
        
        # ì´ë²¤íŠ¸ íƒ€ì…ë³„ ì¶”ê°€ ì •ë³´
        if 'Records' in event:
            error_response['event_type'] = 'SQS'
        elif event.get('source') == 'aws.events':
            error_response['event_type'] = 'EventBridge'
        elif 'httpMethod' in event:
            error_response['event_type'] = 'API_Gateway'
            error_response['method'] = event.get('httpMethod')
            error_response['path'] = event.get('resource', event.get('path'))
        
        # API Gateway ì˜¤ë¥˜ ì‘ë‹µ
        if 'httpMethod' in event:
            return {
                'statusCode': 500,
                'headers': {
                    'Content-Type': 'application/json',
                    'Access-Control-Allow-Origin': '*'
                },
                'body': json.dumps(error_response, ensure_ascii=False)
            }
        
        # ì¼ë°˜ ì˜¤ë¥˜ ì‘ë‹µ
        return {
            'statusCode': 500,
            'body': json.dumps(error_response, ensure_ascii=False)
        }
    
    finally:
        if connection:
            connection.close()


# í…ŒìŠ¤íŠ¸ìš© ë¡œì»¬ ì‹¤í–‰
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì´ë²¤íŠ¸ (SQS ë°©ì‹)
    test_event = {
        "Records": [
            {
                "body": json.dumps(
                    {
                        "Message": json.dumps(
                            {
                                "diaryId": 811,
                                "userId": 2,
                                "emotionVector": [
                                    0.1, 0.7, 0.0, 0.3, 0.0,
                                    0.0, 0.2, 0.0, 0.1, 0.0
                                ],
                                "event": "diary_analyzed",
                            }
                        )
                    }
                )
            }
        ]
    }

    print("í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ê²°ê³¼:")
    result = lambda_handler(test_event, None)
    print(json.dumps(result, indent=2, ensure_ascii=False))